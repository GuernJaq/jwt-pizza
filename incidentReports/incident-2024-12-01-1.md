# Incident: 2024-12-01 05-24-00

## Summary

Two incidents coincided on 12/01/2024, one in which the pizza service was deployed incorrectly and took down all database interaction, and one where the factory service threw errors, the first was resolved at 1:10 PM and the second at 1:24 PM.

A push was made at around 10:00PM, and no errors occurred. During the creation of the service, the load balancer wasn't set up properly, although this didn't become an issue until 5:24 AM when the pizza-service subdomain started returning 500 errors. No functions, metrics or logs worked on the site between 5:24 AM and 1:10 PM.

After the load balancer was manually recreated and DNS updated, the site came back online and revealed that the pizza factory was also encountering errors, meaning no users could purchase pizzas. This was resolved by using the reportURL given in the error message from the factory.

## Detection

All alerts first went off at 5:24 AM, however it was believed alerts were just configured incorrectly and would be fixed in the morning. At around 10:00 the dashboard was manually checked, where it was discovered that the pizza service wasn't working.

For the pizza factory error, as soon as the site came online, the Pizza Purchases metric set off an alert for having failed pizza purchases, and Jaquelyn on call began to fix it immediately.

## Impact

For the service incident, for 7 hours and 46 minutes, between 5:24 AM and 1:10 PM, no users were able to login or call any other functions on the site, simply receiving the error message "500: failed to fetch" when clicking any such buttons.

For the factory incident, for an additional 14 minutes, between 1:10 PM and 1:24 PM, users were unable to make orders, receiving an error "Failed to fulfill order at factory" when trying to order.

## Timeline

All times are MST, some are approximated.

- _22:03_ - Push is made to add a new endpoint to the service, no apparent errors occurred.
- _05:24_ - Grafana metric alerts all trigger due to empty data sent from service.
- _10:00_ - Jaquelyn on call, manually opens the metrics dashboard to check for errors and discovers the outage, begins checking grafana history for outage time.
- _10:20_ - Found complete outage time of 05:24, no apparent causes from grafana, begin checking AWS CloundFormation logs for issue
- _10:40_ - Found downed service in ECS cluster, attempting to recreate task definition
- _10:50_ - Recreating task definition didn't work, attempting to recreate service
- _11:00_ - Recreating service didn't work, attempting to recreate cluster
- _11:20_ - Recreating cluster doesn't resolve issue
- _11:48_ - Attempted redeployment through github push
- _11:50_ - Consulted with fellow developers on issue, unable to resolve
- _12:06_ - Added documentation for new endpoint as possible fix
- _12:16_ - Endpoint completely removed as potential fix, service is still down afterwards.
- _12:30_ - Repeated earlier recreation steps with updated deployment
- _12:50_ - Discovered that the DNS for pizza-service was pointing to a load balancer that no longer existed
- _13:00_ - Load balancer manually recreated
- _13:10_ - DNS updated with new load balancer, site back online and showing pizza purchases are failing, logs show that the factory is returning a 500 error
- _13:20_ - Checked official sites for contact point for pizza-factory
- _13:22_ - Attempting a purchase locally with developer tools console open, discovered "reportURL" returned
- _13:24_ - reportURL used, pizza factory issue resolved.

## Response

Jaquelyn responded to both incidents. When the alert went off at 5:24 AM it was dismissed as a misconfiguration. By 10:00 AM Jaquelyn opened the metrics dashboard to check statuses while on call and discovered the service was down. After consulting with fellow developers and Amazon experts, she was able to determine that the load balancer hadn't been created properly and was able to manually create it to resolve it. 

For the second incident, as Jaquelyn was already using the metrics dashboard to determine whether the service was back online, the issue with the factory was immediately noticed. After manual testing it was able to be resolved quickly.

# Root cause

The root cause for the service incident appears to have been an error in the CloudFront creation stack, where the load balancer for the service wasn't properly created.

The root cause of the factory incident was an error with the factory itself causing it to fail, outside of the range of jwt-pizza developers to fix.

## Resolution

The service issue required extensive log searching and consulation with an Amazon professional to determine possible causes. Error messages were searched, and each solution was tried as it was found, until eventually the missing balancer was discovered and created.

The factory issue was a very quick find, as the logs made it clear that the pizza factory was returning an error, and Jaquelyn just needed to find the correct way to report this issue. 

# Prevention

The service issue was caused by an error in AWS, which can be fixed in the future by checking deployments sooner for errors.

The factory issue was an error on the part of the pizza-factory team, and so we can only recommend they test more before uploading to prevent errors. 

# Action items

1. Double check all deployments after creation to ensure no subtle errors occur
2. Recommend that the pizza factory team test to fix and prevent errors.